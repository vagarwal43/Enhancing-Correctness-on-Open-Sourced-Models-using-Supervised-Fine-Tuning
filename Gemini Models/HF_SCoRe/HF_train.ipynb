{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joncc\\miniconda3\\envs\\mingpt\\lib\\site-packages\\huggingface_hub\\file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'mamba'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [8], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM\n\u001b[0;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate-spaces/mamba-130m-hf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstate-spaces/mamba-130m-hf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\joncc\\miniconda3\\envs\\mingpt\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:461\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    459\u001b[0m     _ \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 461\u001b[0m config, kwargs \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    462\u001b[0m     pretrained_model_name_or_path,\n\u001b[0;32m    463\u001b[0m     return_unused_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    464\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[0;32m    465\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs,\n\u001b[0;32m    466\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    467\u001b[0m )\n\u001b[0;32m    469\u001b[0m \u001b[38;5;66;03m# if torch_dtype=auto was passed here, ensure to pass it on\u001b[39;00m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs_orig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\joncc\\miniconda3\\envs\\mingpt\\lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:998\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    996\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    997\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[1;32m--> 998\u001b[0m     config_class \u001b[38;5;241m=\u001b[39m \u001b[43mCONFIG_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config_class\u001b[38;5;241m.\u001b[39mfrom_dict(config_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwargs)\n\u001b[0;32m   1000\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1001\u001b[0m     \u001b[38;5;66;03m# Fallback: use pattern matching on the string.\u001b[39;00m\n\u001b[0;32m   1002\u001b[0m     \u001b[38;5;66;03m# We go from longer names to shorter names to catch roberta before bert (for instance)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\joncc\\miniconda3\\envs\\mingpt\\lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:710\u001b[0m, in \u001b[0;36m_LazyConfigMapping.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    708\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extra_content[key]\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping:\n\u001b[1;32m--> 710\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[0;32m    711\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping[key]\n\u001b[0;32m    712\u001b[0m module_name \u001b[38;5;241m=\u001b[39m model_type_to_module_name(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'mamba'"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"state-spaces/mamba-130m-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"state-spaces/mamba-130m-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"qintongli/GSM-Plus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_prompt = 'You are a math expert. When you respond, respond only with the Solution of the final Problem, thinking step by step. At the end of the Solution, when you give your final answer, write it in the form \"Final Answer: The final answer is $answer$. I hope it is correct.\"'\n",
    "\n",
    "sc_prompt = 'There might be an error in the solution above because of lack of understanding of the question. Please correct the error, if any, and rewrite the solution. Only output the final solution! At the end of the Solution, when you give your final answer, write it in the form \"Final Answer: The final answer is $answer$. I hope it is correct.\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"state-spaces/mamba-130m-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"state-spaces/mamba-130m-hf\")\n",
    "dataset = load_dataset(\"Abirate/english_quotes\", split=\"train\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-3\n",
    ")\n",
    "lora_config =  LoraConfig(\n",
    "        r=8,\n",
    "        target_modules=[\"x_proj\", \"embeddings\", \"in_proj\", \"out_proj\"],\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        bias=\"none\"\n",
    ")\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    peft_config=lora_config,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"quote\",\n",
    ")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_prompt = 'You are a math expert. When you respond, respond only with the Solution of the final Problem, thinking step by step. At the end of the Solution, when you give your final answer, write it in the form \"Final Answer: The final answer is $answer$. I hope it is correct.\"'\n",
    "\n",
    "sc_prompt = 'There might be an error in the solution above because of lack of understanding of the question. Please correct the error, if any, and rewrite the solution. Only output the final solution! At the end of the Solution, when you give your final answer, write it in the form \"Final Answer: The final answer is $answer$. I hope it is correct.\"'\n",
    "\n",
    "\n",
    "'''\n",
    "load dataset\n",
    "\n",
    "set up inference\n",
    "\n",
    "set up trainer\n",
    "'''\n",
    "# Load configuration from a YAML file\n",
    "def load_config(config_file=None):\n",
    "    if config_file:\n",
    "        with open(config_file, 'r') as f:\n",
    "            config = yaml.safe_load(f)\n",
    "    else:\n",
    "        # Default configuration\n",
    "        config = {\n",
    "            \"model_name\": \"state-spaces/mamba-130m-hf\", \n",
    "            \"learning_rate\": 1e-5,\n",
    "            \"epochs_stage_1\": 2,\n",
    "            \"epochs_stage_2\": 3,\n",
    "            \"beta_kl\": 0.1,\n",
    "            \"alpha\": 1.0,\n",
    "            \"data_file\": \"SCoRe_dataset.csv\"\n",
    "        }\n",
    "    return config\n",
    "\n",
    "# Reward function for self-correction\n",
    "def reward_function(original_answer, corrected_answer, correct_answer):\n",
    "    if corrected_answer == correct_answer:  # Fully correct answer\n",
    "        return 1.0\n",
    "    elif corrected_answer == original_answer:  # No improvement from original answer\n",
    "        return -1.0\n",
    "    else:\n",
    "        return 0.5  # Partial improvement, better than original but still incorrect\n",
    "\n",
    "def first_round_prompt(example):\n",
    "    return [\n",
    "        {\"role\": \"user\", \"content\": example['question']},\n",
    "    ]\n",
    "\n",
    "def second_round_prompt(example, first_round_answer):\n",
    "    return [\n",
    "        {\"role\": \"user\", \"content\": example['question']},\n",
    "        {\"role\": \"assistant\", \"content\": f\"{first_round_answer}\"},\n",
    "        {\"role\": \"user\", \"content\": sc_prompt},\n",
    "    ]\n",
    "\n",
    "# Stage I: Train initial model to generate first attempt (y1) and prevent mode collapse\n",
    "def stage_one_initialization(model, tokenizer, data, epochs=2, lr=1e-5, beta_kl=0.1):\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        for example in data:\n",
    "\n",
    "            # Format input using chat_template\n",
    "            first_round_conversation = first_round_prompt(example)\n",
    "            \n",
    "            # Convert conversation to a single string\n",
    "            conversation_text = tokenizer.apply_chat_template(first_round_conversation, tokenize=False)\n",
    "            \n",
    "            inputs = tokenizer(conversation_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "            \n",
    "            outputs = model(**inputs, labels=inputs['input_ids'])\n",
    "            \n",
    "            # Cross-entropy loss (first attempt)\n",
    "            cross_entropy_loss = outputs.loss\n",
    "            \n",
    "            # Log probabilities and apply KL divergence loss\n",
    "            logits = outputs.logits\n",
    "            log_probs = F.log_softmax(logits, dim=-1)\n",
    "            with torch.no_grad():\n",
    "                target_probs = F.softmax(logits, dim=-1)\n",
    "            kl_loss = F.kl_div(log_probs, target_probs, reduction='batchmean')\n",
    "            \n",
    "            # Total loss combines cross-entropy and scaled KL divergence\n",
    "            total_loss_value = cross_entropy_loss + beta_kl * kl_loss\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            total_loss_value.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += total_loss_value.item()\n",
    "        print(f\"Stage I - Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "def stage1_chat_format(example):\n",
    "    return [\n",
    "        {\"role\": \"user\", \"content\": example['question']},\n",
    "        {\"role\": \"assistant\", \"content\": f\"답변: {example.get('original_answer', '')}\"}\n",
    "    ]\n",
    "\n",
    "def stage_two_training_with_reward_shaping(model, tokenizer, data, epochs=3, lr=1e-5, alpha=1.0):\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        for example in data:\n",
    "            # First attempt (y1): Generate the initial answer using chat_template\n",
    "            conversation1 = stage2_chat_format(example)\n",
    "            conversation_text1 = tokenizer.apply_chat_template(conversation1, tokenize=False)\n",
    "            inputs1 = tokenizer(conversation_text1, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            inputs1 = {k: v.to(model.device) for k, v in inputs1.items()}\n",
    "            \n",
    "            # Generate output for the first attempt\n",
    "            with torch.no_grad():\n",
    "                outputs1 = model(**inputs1)\n",
    "            \n",
    "            # Second attempt (y2): Corrected answer\n",
    "            conversation2 = chat_format(example)\n",
    "            conversation_text2 = tokenizer.apply_chat_template(conversation2, tokenize=False)\n",
    "            inputs2 = tokenizer(conversation_text2, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            inputs2 = {k: v.to(model.device) for k, v in inputs2.items()}\n",
    "            \n",
    "            # Forward pass with labels for loss calculation\n",
    "            outputs2 = model(**inputs2, labels=inputs2['input_ids'])\n",
    "            \n",
    "            # Ensure we have a loss\n",
    "            if outputs2.loss is None:\n",
    "                print(\"Warning: Model output does not include loss. Using cross-entropy loss.\")\n",
    "                logits = outputs2.logits\n",
    "                loss = F.cross_entropy(logits.view(-1, logits.size(-1)), inputs2['input_ids'].view(-1))\n",
    "            else:\n",
    "                loss = outputs2.loss\n",
    "            \n",
    "            # Compute reward based on self-correction\n",
    "            generated_text1 = tokenizer.decode(outputs1.logits.argmax(dim=-1)[0], skip_special_tokens=True)\n",
    "            generated_text2 = tokenizer.decode(outputs2.logits.argmax(dim=-1)[0], skip_special_tokens=True)\n",
    "            reward = reward_function(generated_text1, generated_text2, example.get('correct_answer', ''))\n",
    "            \n",
    "            # Apply reward shaping\n",
    "            shaped_loss = loss * reward\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            shaped_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += shaped_loss.item()\n",
    "        print(f\"Stage II - Epoch {epoch+1}, Total Loss: {total_loss:.4f}\")\n",
    "\n",
    "def stage2_chat_format(example):\n",
    "    return [\n",
    "        {\"role\": \"user\", \"content\": example['question']},\n",
    "        {\"role\": \"assistant\", \"content\": f\"첫 번째 답변: {example.get('original_answer', '')}\"},\n",
    "        {\"role\": \"user\", \"content\": \"이 답변을 다시 한 번 검토해주세요.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"검토 후 답변: \"}\n",
    "    ]\n",
    "\n",
    "def chat_format(example):\n",
    "    return [\n",
    "        {\"role\": \"user\", \"content\": example['question']},\n",
    "        {\"role\": \"assistant\", \"content\": f\"최종 답변: {example.get('correct_answer', '')}\"}\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Main function to run the training process\n",
    "def main(config_file=None):\n",
    "    config = load_config(config_file)\n",
    "\n",
    "    # Load model and tokenizer\n",
    "    model_name = config[\"model_name\"]\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name, \n",
    "                                        device_map=\"auto\", \n",
    "                                        attn_implementation='eager')\n",
    "\n",
    "    # Load the dataset\n",
    "    data_file_path = config[\"data_file\"]\n",
    "    df = pd.read_csv(data_file_path)\n",
    "\n",
    "    # Prepare the data for Stage I and Stage II\n",
    "    data_stage_one = df[[\"question\", \"original_answer\"]].to_dict(orient=\"records\")\n",
    "    data_stage_two = df[[\"question\", \"original_answer\", \"correct_answer\"]].to_dict(orient=\"records\")\n",
    "\n",
    "    # Stage I training (Initialization)\n",
    "    stage_one_initialization(\n",
    "        model, tokenizer, data_stage_one, \n",
    "        epochs=config[\"epochs_stage_1\"], \n",
    "        lr=config[\"learning_rate\"], \n",
    "        beta_kl=config[\"beta_kl\"]\n",
    "    )\n",
    "\n",
    "    # Stage II training (Self-correction)\n",
    "    stage_two_training_with_reward_shaping(\n",
    "        model, tokenizer, data_stage_two, \n",
    "        epochs=config[\"epochs_stage_2\"], \n",
    "        lr=config[\"learning_rate\"], \n",
    "        alpha=config[\"alpha\"]\n",
    "    )\n",
    "\n",
    "    # Save the trained model\n",
    "    model.save_pretrained(\"./trained_self_correcting_model\")\n",
    "    tokenizer.save_pretrained(\"./trained_self_correcting_model\")\n",
    "\n",
    "# Run the main function (can use a config file or default)\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--config', type=str, help='Path to config file', default=None)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    main(args.config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mingpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
